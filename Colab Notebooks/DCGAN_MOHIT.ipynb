{"cells":[{"cell_type":"markdown","source":["## E0270 Machine Learning Course Project\n","# DCGAN implementation\n","Name : Mohit Kumar<br>\n","M. Tech in Artificial Intelligence<br>\n","SR No. : 04-01-03-10-51-21-1-19825<br>\n","email : mohitk2@iisc.ac.in<br>\n","\n","I learnt using the pytorch library from [PyTorch Tutorials by Aladdin Persson](https://www.youtube.com/playlist?list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz)\n","\n","For implementing the DCGAN I took help from the YouTube video [DCGAN implementation from scratch](https://www.youtube.com/watch?v=IZtv9s_Wx9I&list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&index=3)<br>\n","and the official DCGAN paper [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434) by Alec Radford, Luke Metz and Soumith Chintala\n","  "],"metadata":{"id":"uTyy98LeOpKL"}},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6180422-a419-47ed-acbc-93c9b0722cc2","_kg_hide-input":true,"_uuid":"f7601d03-a483-410b-b136-e2fd99bb0c9a","colab":{"base_uri":"https://localhost:8080/"},"id":"Cj_972D1bRub","executionInfo":{"status":"ok","timestamp":1651066780572,"user_tz":-330,"elapsed":42642,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}},"outputId":"4c4d7201-d5d7-4596-c50b-887cfc2f1f65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/My Drive/GAN\n","checkpoint  dataset  outputs\n"]}],"source":["# imports\n","\n","from __future__ import print_function\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import torchvision.models as models\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import pandas as pd\n","import torchvision\n","import sys\n","from scipy import linalg\n","from torch.nn.functional import adaptive_avg_pool2d\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/'My Drive'/GAN/\n","!ls"]},{"cell_type":"markdown","source":["## FID implementation\n","Code taken from [FID score for PyTorch](https://github.com/mseitzer/pytorch-fid)<br>\n","<img src='https://drive.google.com/uc?id=11r3lyk-PUkOzSV5LFZeeZ1AYGVB7QMDa'  height='450' width='700' ><br>\n","[image source](https://www.kaggle.com/code/ibtesama/gan-in-pytorch-with-fid/notebook)"],"metadata":{"id":"2NsRr1YVJ40A"}},{"cell_type":"code","source":["class InceptionV3(nn.Module):\n","    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n","\n","    # Index of default block of inception to return,\n","    # corresponds to output of final average pooling\n","    DEFAULT_BLOCK_INDEX = 3\n","\n","    # Maps feature dimensionality to their output blocks indices\n","    BLOCK_INDEX_BY_DIM = {\n","        64: 0,   # First max pooling features\n","        192: 1,  # Second max pooling featurs\n","        768: 2,  # Pre-aux classifier features\n","        2048: 3  # Final average pooling features\n","    }\n","\n","    def __init__(self,\n","                 output_blocks=[DEFAULT_BLOCK_INDEX],\n","                 resize_input=True,\n","                 normalize_input=True,\n","                 requires_grad=False):\n","        \n","        super(InceptionV3, self).__init__()\n","\n","        self.resize_input = resize_input\n","        self.normalize_input = normalize_input\n","        self.output_blocks = sorted(output_blocks)\n","        self.last_needed_block = max(output_blocks)\n","\n","        assert self.last_needed_block <= 3, \\\n","            'Last possible output block index is 3'\n","\n","        self.blocks = nn.ModuleList()\n","\n","        \n","        inception = models.inception_v3(pretrained=True)\n","\n","        # Block 0: input to maxpool1\n","        block0 = [\n","            inception.Conv2d_1a_3x3,\n","            inception.Conv2d_2a_3x3,\n","            inception.Conv2d_2b_3x3,\n","            nn.MaxPool2d(kernel_size=3, stride=2)\n","        ]\n","        self.blocks.append(nn.Sequential(*block0))\n","\n","        # Block 1: maxpool1 to maxpool2\n","        if self.last_needed_block >= 1:\n","            block1 = [\n","                inception.Conv2d_3b_1x1,\n","                inception.Conv2d_4a_3x3,\n","                nn.MaxPool2d(kernel_size=3, stride=2)\n","            ]\n","            self.blocks.append(nn.Sequential(*block1))\n","\n","        # Block 2: maxpool2 to aux classifier\n","        if self.last_needed_block >= 2:\n","            block2 = [\n","                inception.Mixed_5b,\n","                inception.Mixed_5c,\n","                inception.Mixed_5d,\n","                inception.Mixed_6a,\n","                inception.Mixed_6b,\n","                inception.Mixed_6c,\n","                inception.Mixed_6d,\n","                inception.Mixed_6e,\n","            ]\n","            self.blocks.append(nn.Sequential(*block2))\n","\n","        # Block 3: aux classifier to final avgpool\n","        if self.last_needed_block >= 3:\n","            block3 = [\n","                inception.Mixed_7a,\n","                inception.Mixed_7b,\n","                inception.Mixed_7c,\n","                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","            ]\n","            self.blocks.append(nn.Sequential(*block3))\n","\n","        for param in self.parameters():\n","            param.requires_grad = requires_grad\n","\n","    def forward(self, inp):\n","        \"\"\"Get Inception feature maps\n","        Parameters\n","        ----------\n","        inp : torch.autograd.Variable\n","            Input tensor of shape Bx3xHxW. Values are expected to be in\n","            range (0, 1)\n","        Returns\n","        -------\n","        List of torch.autograd.Variable, corresponding to the selected output\n","        block, sorted ascending by index\n","        \"\"\"\n","        outp = []\n","        x = inp\n","\n","        if self.resize_input:\n","            x = F.interpolate(x,\n","                              size=(299, 299),\n","                              mode='bilinear',\n","                              align_corners=False)\n","\n","        if self.normalize_input:\n","            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n","\n","        for idx, block in enumerate(self.blocks):\n","            x = block(x)\n","            if idx in self.output_blocks:\n","                outp.append(x)\n","\n","            if idx == self.last_needed_block:\n","                break\n","\n","        return outp\n","    \n","block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n","model = InceptionV3([block_idx])\n","model=model.cuda()"],"metadata":{"id":"ySCdKyC6Fsx0","executionInfo":{"status":"ok","timestamp":1651066792360,"user_tz":-330,"elapsed":11811,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["1583d09dce66419b8feeffa28cc1e97f","7c50df438bcd4dedaf7f60b017e07012","69a8d26081b2448eb1dbd4bc8d6cb970","faf7e3bc324742f6b1ca34840300f936","a7aff33a77c34ca2b5b7dbf640288285","c9bacb4a7ae942edb3dbaebdb95de6c3","23e63ed00ea04e61aa4a190672de712b","932d6a6ef7a1462e9329376f962c8fb0","893dcc7c071744ce89f3be6b26595b5c","cd250527594d40fab0b1bb9c66f6f5a2","deaa062c10724e0eb3ec496efbd2df97"]},"outputId":"1b458eab-4daf-4ce6-b1d4-5116e22fc986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/104M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1583d09dce66419b8feeffa28cc1e97f"}},"metadata":{}}]},{"cell_type":"code","source":["def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n","                    cuda=False):\n","    model.eval()\n","    act=np.empty((len(images), dims))\n","    \n","    if cuda:\n","        batch=images.cuda()\n","    else:\n","        batch=images\n","    pred = model(batch)[0]\n","\n","        # If model output is not scalar, apply global spatial average pooling.\n","        # This happens if you choose a dimensionality not equal 2048.\n","    if pred.size(2) != 1 or pred.size(3) != 1:\n","        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n","\n","    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n","    \n","    mu = np.mean(act, axis=0)\n","    sigma = np.cov(act, rowvar=False)\n","    return mu, sigma"],"metadata":{"id":"danLVYDEF2CW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    \"\"\"Numpy implementation of the Frechet Distance.\n","    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n","    and X_2 ~ N(mu_2, C_2) is\n","            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n","    \"\"\"\n","\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, \\\n","        'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, \\\n","        'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","\n","    \n","    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = ('fid calculation produces singular product; '\n","               'adding %s to diagonal of cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","\n","    \n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return (diff.dot(diff) + np.trace(sigma1) +\n","            np.trace(sigma2) - 2 * tr_covmean)"],"metadata":{"id":"N_F6Rq6FF7cO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_fretchet(images_real,images_fake,model):\n","     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n","     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n","    \n","     \"\"\"get fretched distance\"\"\"\n","     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n","     return fid_value"],"metadata":{"id":"g3Y_P26lGEjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Functions for loading and saving checkpoints"],"metadata":{"id":"f6YRycu5TA7h"}},{"cell_type":"code","source":["# function for saving the state of the trained discriminator and generator\n","def save_checkpoint(state, filename=\"cifar10_dcgan.pth.tar\"): \n","    if not os.path.exists(\"./checkpoint/{}/\".format('dcgan')):\n","        os.makedirs(\"./checkpoint/{}/\".format('dcgan'))\n","    print(\"=> Saving checkpoint....\")\n","    torch.save(state, os.path.join(\"./checkpoint/{}/\".format('dcgan'),filename))\n","\n","# function for loading the state of the trained discriminator and generator\n","def load_checkpoint(checkpoint, gen, disc):\n","    print(\"=> Loading checkpoint....\")\n","    gen.load_state_dict(checkpoint['gen'])\n","    disc.load_state_dict(checkpoint['disc'])"],"metadata":{"id":"FnyBKaJIK5wn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definition of the Discriminator and Generator Networks\n","<img src='https://drive.google.com/uc?id=1AfOR3_glHI_WHJ3HJ1wglH7Te5R6NHvP' height='200' width='600'><br>[image source](https://arxiv.org/abs/1511.06434)"],"metadata":{"id":"qXCB92aUTPpf"}},{"cell_type":"code","source":["# discriminator network\n","class Discriminator(nn.Module): \n","    def __init__(self, img_channels, disc_features):\n","        super(Discriminator, self).__init__()\n","        self.d = nn.Sequential(\n","            # Input: N x img_channels x disc_features x disc_features (N X 3 X 64 X 64)\n","            nn.Conv2d(\n","                img_channels, disc_features, kernel_size=4, stride=2, padding=1\n","            ), # 32 X 32\n","            nn.LeakyReLU(0.2),\n","            self._block(disc_features, disc_features * 2, 4, 2, 1), # 16 X 16\n","            self._block(disc_features * 2, disc_features * 4, 4, 2, 1), # 8 X 8\n","            self._block(disc_features * 4, disc_features * 8, 4, 2, 1), # 4 X 4\n","            nn.Conv2d(disc_features * 8, 1, kernel_size=4, stride=2, padding=0), # 1 X 1\n","            nn.Sigmoid(),\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x):\n","        return self.d(x)\n","\n","#  generator network \n","class Generator(nn.Module):\n","    def __init__(self, z_dim, img_channels, gen_features):\n","        super(Generator, self).__init__()\n","        self.g = nn.Sequential(\n","            # Input: N x z_dim x 1 x 1\n","            self._block(z_dim, gen_features * 16, 4, 1, 0), # 4 x 4\n","            self._block(gen_features * 16, gen_features * 8, 4, 2, 1), # 8 x 8\n","            self._block(gen_features * 8, gen_features * 4, 4, 2, 1), # 16 x 16\n","            self._block(gen_features * 4, gen_features * 2, 4, 2, 1), # 32  x32\n","            nn.ConvTranspose2d(gen_features * 2, img_channels, kernel_size=4, stride=2, padding=1),\n","            # Output: N x 3 x 64 x 64\n","            nn.Tanh(),\n","        )\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride,\n","                padding,\n","                bias=False,\n","            ),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, x):\n","        return self.g(x)\n","\n","# function for initializing weights of the neural network layers\n","def initialize_weights(model):\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)"],"metadata":{"id":"fM37kHQ5bY_4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training of the DCGAN\n","<img src='https://drive.google.com/uc?id=1exRpZ8b74TjlTEe5wU5RIwJU2WWqIK68' width='600' height='210'><br>\n","[image source](https://cedar.buffalo.edu/~srihari/CSE676/index.html)"],"metadata":{"id":"z0WdHwezVdvE"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","channels_img = 3\n","batch_size = 128\n","img_size = 64\n","num_epochs = 100\n","disc_features = 64\n","gen_features = 64\n","disc_lr = 0.0004 \n","gen_lr = 0.0001\n","z_dim = 100\n","\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]\n","        ),\n","    ]\n",")\n","\n","\n","dataset = datasets.CIFAR10(root=\"dataset/\", train=True, transform=transforms,download=True)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","gen = Generator(z_dim, channels_img, gen_features).to(device)\n","print(gen)\n","disc = Discriminator(channels_img, disc_features).to(device)\n","print(disc)\n","\n","initialize_weights(disc)\n","initialize_weights(gen)\n","\n","opt_gen = optim.Adam(gen.parameters(), lr=gen_lr, betas=(0.5, 0.999))\n","opt_disc = optim.Adam(disc.parameters(), lr=disc_lr, betas=(0.5, 0.999))\n","criterion = nn.BCELoss()\n","\n","if os.path.exists(os.path.join(\"./checkpoint/{}/\".format('dcgan'), \"cifar10_dcgan.pth.tar\")):\n","    checkpoint = torch.load(os.path.join(\"./checkpoint/{}/\".format('dcgan'), \"cifar10_dcgan.pth.tar\"))\n","    load_checkpoint(checkpoint, gen, disc)\n","\n","step = 0\n","\n","gen.train()\n","disc.train()\n","\n","Gen_losses = []\n","Disc_losses = []\n","fretchet_distances=[]\n","iter_list = []\n","\n","print(\"Starting Training Loop....\")\n","\n","for epoch in range(num_epochs):\n","    if epoch % 5 == 0:\n","        checkpoint = {'gen' : gen.state_dict(), 'disc' : disc.state_dict()}\n","        save_checkpoint(checkpoint)\n","\n","    for batch_index, (real, _) in enumerate(loader):\n","        real = real.to(device)\n","        noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n","        fake = gen(noise)\n","\n","        # Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n","        disc_real = disc(real).reshape(-1)\n","        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n","        disc_fake = disc(fake).reshape(-1)\n","        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n","        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n","        disc.zero_grad()\n","        loss_disc.backward(retain_graph=True)\n","        opt_disc.step()\n","\n","        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n","        output = disc(fake).reshape(-1)\n","        loss_gen = criterion(output, torch.ones_like(output))\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        # Print losses \n","        if batch_index % 100 == 0:\n","            print(\n","                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_index}/{len(loader)} \\\n","                  Disc Loss : {loss_disc:.4f}, Gen loss : {loss_gen:.4f}\"\n","            )\n","        \n","        # save images generated by the generator, generator and discriminator losses and fid score every 500 iterations\n","        if step % 500 == 0 and step > 0:\n","            with torch.no_grad():\n","                fake = gen(noise)\n","                img_grid_fake = torchvision.utils.make_grid(fake[:64], normalize=True)\n","                if not os.path.exists(\"./outputs/{}/generated_images/\".format('dcgan')):\n","                    os.makedirs(\"./outputs/{}/generated_images/\".format('dcgan'))\n","                torchvision.utils.save_image(img_grid_fake, \"./outputs/{}/generated_images/img_{}.png\".format('dcgan', str(step)),)\n","            iter_list.append(step)\n","            Gen_losses.append(loss_gen.item())\n","            Disc_losses.append(loss_disc.item())\n","            fretchet_distance=calculate_fretchet(real,fake,model)\n","            fretchet_distances.append(fretchet_distance.item())\n","\n","            losses = pd.DataFrame({\"Iteration No.\": iter_list, \"Generator Loss\": Gen_losses, \"Discriminator Loss\": Disc_losses,\"FID Score\": fretchet_distances})\n","\n","            if not os.path.exists(\"./outputs/{}/\".format('dcgan')):\n","                os.makedirs(\"./outputs/{}/\".format('dcgan'))\n","            losses.to_csv(\"./outputs/{}/output_data.csv\".format('dcgan'))\n","\n","        step += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AyCuKmHlsXM","outputId":"edd11cd3-de9d-476d-d2fa-c2b774e59a5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Generator(\n","  (g): Sequential(\n","    (0): Sequential(\n","      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (1): Sequential(\n","      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (2): Sequential(\n","      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (3): Sequential(\n","      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (5): Tanh()\n","  )\n",")\n","Discriminator(\n","  (d): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (3): Sequential(\n","      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n","    (6): Sigmoid()\n","  )\n",")\n","=> Loading checkpoint....\n","Starting Training Loop....\n","=> Saving checkpoint....\n","Epoch [0/100] Batch 0/391                   Disc Loss : 0.0006, Gen loss : 16.8838\n","Epoch [0/100] Batch 100/391                   Disc Loss : 0.0000, Gen loss : 12.0622\n","Epoch [0/100] Batch 200/391                   Disc Loss : 0.0010, Gen loss : 13.9233\n","Epoch [0/100] Batch 300/391                   Disc Loss : 0.0059, Gen loss : 12.1151\n","Epoch [1/100] Batch 0/391                   Disc Loss : 0.5192, Gen loss : 46.4597\n","Epoch [1/100] Batch 100/391                   Disc Loss : 0.0018, Gen loss : 8.3356\n","Epoch [1/100] Batch 200/391                   Disc Loss : 0.0037, Gen loss : 9.9084\n","Epoch [1/100] Batch 300/391                   Disc Loss : 0.0010, Gen loss : 7.2958\n","Epoch [2/100] Batch 0/391                   Disc Loss : 0.0039, Gen loss : 7.4593\n"]}]},{"cell_type":"code","source":["# loading data from output_data.csv for plotting graphs \n","df = pd.read_csv(\"./outputs/{}/output_data.csv\".format('dcgan'))\n","Iterations = df['Iteration No.']\n","Gen_Loss = df['Generator Loss']\n","Disc_Loss = df['Discriminator Loss']\n","FID = df['FID Score']"],"metadata":{"id":"dGZFY48Vn2T9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the generator losses\n","plt.xlabel('Iterations')\n","plt.ylabel('Generator Loss')\n","plt.plot(Iterations, Gen_Loss)"],"metadata":{"id":"vMgst9I9qgnl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the discriminator losses\n","plt.xlabel('Iterations')\n","plt.ylabel('Discriminator Loss')\n","plt.plot(Iterations, Disc_Loss)"],"metadata":{"id":"s6iNWfFGqk0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the FID scores\n","plt.xlabel('Iterations')\n","plt.ylabel('FID Score')\n","plt.plot(Iterations, FID)"],"metadata":{"id":"n0AlMOJSqsIY"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"DCGAN_MOHIT.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1583d09dce66419b8feeffa28cc1e97f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c50df438bcd4dedaf7f60b017e07012","IPY_MODEL_69a8d26081b2448eb1dbd4bc8d6cb970","IPY_MODEL_faf7e3bc324742f6b1ca34840300f936"],"layout":"IPY_MODEL_a7aff33a77c34ca2b5b7dbf640288285"}},"7c50df438bcd4dedaf7f60b017e07012":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9bacb4a7ae942edb3dbaebdb95de6c3","placeholder":"​","style":"IPY_MODEL_23e63ed00ea04e61aa4a190672de712b","value":"100%"}},"69a8d26081b2448eb1dbd4bc8d6cb970":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_932d6a6ef7a1462e9329376f962c8fb0","max":108949747,"min":0,"orientation":"horizontal","style":"IPY_MODEL_893dcc7c071744ce89f3be6b26595b5c","value":108949747}},"faf7e3bc324742f6b1ca34840300f936":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd250527594d40fab0b1bb9c66f6f5a2","placeholder":"​","style":"IPY_MODEL_deaa062c10724e0eb3ec496efbd2df97","value":" 104M/104M [00:01&lt;00:00, 57.9MB/s]"}},"a7aff33a77c34ca2b5b7dbf640288285":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bacb4a7ae942edb3dbaebdb95de6c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23e63ed00ea04e61aa4a190672de712b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"932d6a6ef7a1462e9329376f962c8fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"893dcc7c071744ce89f3be6b26595b5c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd250527594d40fab0b1bb9c66f6f5a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deaa062c10724e0eb3ec496efbd2df97":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}