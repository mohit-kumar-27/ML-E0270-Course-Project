{"cells":[{"cell_type":"markdown","source":["## E0270 Machine Learning Course Project\n","# SAGAN implementation\n","Name : Mohit Kumar<br>\n","M. Tech in Artificial Intelligence<br>\n","SR No. : 04-01-03-10-51-21-1-19825<br>\n","email : mohitk2@iisc.ac.in<br>\n","\n","I learnt using the pytorch library from [PyTorch Tutorials by Aladdin Persson](https://www.youtube.com/playlist?list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz)\n","\n","For implementing the SAGAN I took help from the YouTube videos<br> [PyTorch Conditional GAN Tutorial](https://www.youtube.com/watch?v=Hp-jWm2SzR8&list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&index=5&t=207s)<br>\n","[WGAN implementation from scratch(with gradient penalty)](https://www.youtube.com/watch?v=pG0QZ7OddX4&list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&index=4&t=1370s)<br>\n","and the official SAGAN paper [Self-Attention Generative Adversarial Networks](https://proceedings.mlr.press/v97/zhang19d.html) by Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena"],"metadata":{"id":"mpZ1UDLX1odM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37591,"status":"ok","timestamp":1651076760129,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"},"user_tz":-330},"id":"WwPNyRGW2Hxr","outputId":"beec3b7c-f47a-419b-b053-7e493244288c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/My Drive/GAN\n","checkpoint  dataset  outputs\n"]}],"source":["# imports\n","from __future__ import print_function\n","import argparse\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision.models as models\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","import pandas as pd\n","import torchvision\n","import time\n","from scipy import linalg\n","from torch.nn.functional import adaptive_avg_pool2d\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import sys\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","%cd /content/drive/'My Drive'/GAN/\n","!ls"]},{"cell_type":"markdown","source":["## FID implementation\n","Code taken from [FID score for PyTorch](https://github.com/mseitzer/pytorch-fid)<br>\n","<img src='https://drive.google.com/uc?id=11r3lyk-PUkOzSV5LFZeeZ1AYGVB7QMDa'  height='450' width='700' ><br>\n","[image source](https://www.kaggle.com/code/ibtesama/gan-in-pytorch-with-fid/notebook)"],"metadata":{"id":"omZLY3jT46gs"}},{"cell_type":"code","source":["class InceptionV3(nn.Module):\n","    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n","\n","    # Index of default block of inception to return,\n","    # corresponds to output of final average pooling\n","    DEFAULT_BLOCK_INDEX = 3\n","\n","    # Maps feature dimensionality to their output blocks indices\n","    BLOCK_INDEX_BY_DIM = {\n","        64: 0,   # First max pooling features\n","        192: 1,  # Second max pooling featurs\n","        768: 2,  # Pre-aux classifier features\n","        2048: 3  # Final average pooling features\n","    }\n","\n","    def __init__(self,\n","                 output_blocks=[DEFAULT_BLOCK_INDEX],\n","                 resize_input=True,\n","                 normalize_input=True,\n","                 requires_grad=False):\n","        \n","        super(InceptionV3, self).__init__()\n","\n","        self.resize_input = resize_input\n","        self.normalize_input = normalize_input\n","        self.output_blocks = sorted(output_blocks)\n","        self.last_needed_block = max(output_blocks)\n","\n","        assert self.last_needed_block <= 3, \\\n","            'Last possible output block index is 3'\n","\n","        self.blocks = nn.ModuleList()\n","\n","        \n","        inception = models.inception_v3(pretrained=True)\n","\n","        # Block 0: input to maxpool1\n","        block0 = [\n","            inception.Conv2d_1a_3x3,\n","            inception.Conv2d_2a_3x3,\n","            inception.Conv2d_2b_3x3,\n","            nn.MaxPool2d(kernel_size=3, stride=2)\n","        ]\n","        self.blocks.append(nn.Sequential(*block0))\n","\n","        # Block 1: maxpool1 to maxpool2\n","        if self.last_needed_block >= 1:\n","            block1 = [\n","                inception.Conv2d_3b_1x1,\n","                inception.Conv2d_4a_3x3,\n","                nn.MaxPool2d(kernel_size=3, stride=2)\n","            ]\n","            self.blocks.append(nn.Sequential(*block1))\n","\n","        # Block 2: maxpool2 to aux classifier\n","        if self.last_needed_block >= 2:\n","            block2 = [\n","                inception.Mixed_5b,\n","                inception.Mixed_5c,\n","                inception.Mixed_5d,\n","                inception.Mixed_6a,\n","                inception.Mixed_6b,\n","                inception.Mixed_6c,\n","                inception.Mixed_6d,\n","                inception.Mixed_6e,\n","            ]\n","            self.blocks.append(nn.Sequential(*block2))\n","\n","        # Block 3: aux classifier to final avgpool\n","        if self.last_needed_block >= 3:\n","            block3 = [\n","                inception.Mixed_7a,\n","                inception.Mixed_7b,\n","                inception.Mixed_7c,\n","                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","            ]\n","            self.blocks.append(nn.Sequential(*block3))\n","\n","        for param in self.parameters():\n","            param.requires_grad = requires_grad\n","\n","    def forward(self, inp):\n","        \"\"\"Get Inception feature maps\n","        Parameters\n","        ----------\n","        inp : torch.autograd.Variable\n","            Input tensor of shape Bx3xHxW. Values are expected to be in\n","            range (0, 1)\n","        Returns\n","        -------\n","        List of torch.autograd.Variable, corresponding to the selected output\n","        block, sorted ascending by index\n","        \"\"\"\n","        outp = []\n","        x = inp\n","\n","        if self.resize_input:\n","            x = F.interpolate(x,\n","                              size=(299, 299),\n","                              mode='bilinear',\n","                              align_corners=False)\n","\n","        if self.normalize_input:\n","            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n","\n","        for idx, block in enumerate(self.blocks):\n","            x = block(x)\n","            if idx in self.output_blocks:\n","                outp.append(x)\n","\n","            if idx == self.last_needed_block:\n","                break\n","\n","        return outp\n","    \n","block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n","model = InceptionV3([block_idx])\n","model=model.cuda()"],"metadata":{"id":"RT1-J78tLlq-","executionInfo":{"status":"ok","timestamp":1651076770221,"user_tz":-330,"elapsed":10105,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["83f5f80d96ae439ab892fbb70038dc23","ed8c2b83d97e4779bb90e92b259e2158","2a34e5fc98a948a1970fb3e21312ead6","b393f1c2c7ff436cbe6ad16de656f6cd","765ebc318005472fadee09bf13b98d4b","de641ad109914e16a62aea2fd9d30ec2","d09379a25ec84e758bc2582be69869e2","1f4ae6c2c8aa48ab96476fc7b2e12d4d","6120c46e4dae4dd5ab409e62768d02c5","2823982b920a4b35bd6b6b79ae02c6cc","a2321e34f376479ea4462896f7c5bfe0"]},"outputId":"c9b4fb66-44cf-4205-b787-8846182c78c8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/104M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f5f80d96ae439ab892fbb70038dc23"}},"metadata":{}}]},{"cell_type":"code","source":["def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n","                    cuda=False):\n","    model.eval()\n","    act=np.empty((len(images), dims))\n","    \n","    if cuda:\n","        batch=images.cuda()\n","    else:\n","        batch=images\n","    pred = model(batch)[0]\n","\n","        # If model output is not scalar, apply global spatial average pooling.\n","        # This happens if you choose a dimensionality not equal 2048.\n","    if pred.size(2) != 1 or pred.size(3) != 1:\n","        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n","\n","    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n","    \n","    mu = np.mean(act, axis=0)\n","    sigma = np.cov(act, rowvar=False)\n","    return mu, sigma"],"metadata":{"id":"xumwAmhOLsqg","executionInfo":{"status":"ok","timestamp":1651076770222,"user_tz":-330,"elapsed":25,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    \"\"\"Numpy implementation of the Frechet Distance.\n","    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n","    and X_2 ~ N(mu_2, C_2) is\n","            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n","    \"\"\"\n","\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, \\\n","        'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, \\\n","        'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","\n","    \n","    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = ('fid calculation produces singular product; '\n","               'adding %s to diagonal of cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","\n","    \n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return (diff.dot(diff) + np.trace(sigma1) +\n","            np.trace(sigma2) - 2 * tr_covmean)"],"metadata":{"id":"WQkWi6DvL15N","executionInfo":{"status":"ok","timestamp":1651076770223,"user_tz":-330,"elapsed":23,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def calculate_fretchet(images_real,images_fake,model):\n","     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n","     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n","    \n","     \"\"\"get fretched distance\"\"\"\n","     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n","     return fid_value"],"metadata":{"id":"jdh3oJmPMA1E","executionInfo":{"status":"ok","timestamp":1651076770223,"user_tz":-330,"elapsed":20,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Function for calculating the gradient penalty"],"metadata":{"id":"8hwfDLVW6WMs"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"lukQpsZJGkWQ","executionInfo":{"status":"ok","timestamp":1651076770224,"user_tz":-330,"elapsed":19,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"outputs":[],"source":["def gradient_penalty(critic, labels, real, fake, device=\"gpu\"):\n","    batch_size, C, H, W = real.shape\n","    alpha = torch.rand((batch_size, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * alpha + fake * (1 - alpha)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, labels)\n","\n","    # Calculate the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty"]},{"cell_type":"markdown","source":["Functions for loading and saving checkpoints"],"metadata":{"id":"M2xIArN36nTL"}},{"cell_type":"code","source":["# function for saving the state of the trained discriminator and generator\n","def save_checkpoint(state, filename=\"cifar10_sagan.pth.tar\"):\n","    if not os.path.exists(\"./checkpoint/{}/\".format('sagan')):\n","        os.makedirs(\"./checkpoint/{}/\".format('sagan'))\n","    print(\"=> Saving checkpoint....\")\n","    torch.save(state, os.path.join(\"./checkpoint/{}/\".format('sagan'),filename))\n","\n","# function for loading the state of the trained discriminator and generator\n","def load_checkpoint(checkpoint, gen, critic):\n","    print(\"=> Loading checkpoint....\")\n","    gen.load_state_dict(checkpoint['gen'])\n","    critic.load_state_dict(checkpoint['critic'])"],"metadata":{"id":"hJecNgQ75tuH","executionInfo":{"status":"ok","timestamp":1651076770226,"user_tz":-330,"elapsed":20,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Self Attention Layer\n","<img src='https://drive.google.com/uc?id=1diWtHE9iXa6z2kmgUa4qTweXhqxCqr1A'  height='225' width='600' ><br>\n","[image source](https://proceedings.mlr.press/v97/zhang19d.html\\)\n","\n"],"metadata":{"id":"3IVY3qJ69nXP"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"EljGGq-Rsmah","executionInfo":{"status":"ok","timestamp":1651076770226,"user_tz":-330,"elapsed":19,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"outputs":[],"source":["import numpy as np\n","import torch.nn.functional as F\n","from torch.nn.utils import spectral_norm\n","from torch.nn.init import xavier_uniform_\n","\n","# function for applying spectral normalization on the top of convolutional layer\n","def snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n","    return spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n","                                   stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n","\n","# Self Attention Layer \n","class Self_Attention_Layer(nn.Module):\n","    def __init__(self, in_channels):\n","        super(Self_Attention_Layer, self).__init__()\n","        self.in_channels = in_channels\n","        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n","        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n","        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, stride=1, padding=0)\n","        self.snconv1x1_attn = snconv2d(in_channels=in_channels//2, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n","        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n","        self.softmax  = nn.Softmax(dim=-1)\n","        self.sigma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","    # input feature map x (B X C X W X H)                          \n","    # out : self attention value + input feature \n","    # attention: B X N X N (N is Width*Height)\n","        \n","        _, ch, h, w = x.size()\n","        # Theta path\n","        theta = self.snconv1x1_theta(x)\n","        theta = theta.view(-1, ch//8, h*w)\n","        # Phi path\n","        phi = self.snconv1x1_phi(x)\n","        phi = self.maxpool(phi)\n","        phi = phi.view(-1, ch//8, h*w//4)\n","        # Attention map\n","        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n","        attn = self.softmax(attn)\n","        # g path\n","        g = self.snconv1x1_g(x)\n","        g = self.maxpool(g)\n","        g = g.view(-1, ch//2, h*w//4)\n","        # Attention_g\n","        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n","        attn_g = attn_g.view(-1, ch//2, h, w)\n","        attn_g = self.snconv1x1_attn(attn_g)\n","        # Output\n","        out = x + self.sigma*attn_g\n","        return out"]},{"cell_type":"markdown","source":["## Definition of the Discriminator and Generator Networks\n","<img src='https://drive.google.com/uc?id=1FMI_cN563HG8eT4vS8P5V0AEm7BI9wBs'  height='500' width='450' ><br>\n","**Conditional GAN**<br>\n","[image source](https://cedar.buffalo.edu/~srihari/CSE676/index.html)\n"],"metadata":{"id":"yKPHkpDo9wFc"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"Av_6oh0aGpv_","executionInfo":{"status":"ok","timestamp":1651076770227,"user_tz":-330,"elapsed":16,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# discriminator network\n","class Discriminator(nn.Module):\n","    def __init__(self, img_channels, features_d, num_classes, img_size):\n","        super(Discriminator, self).__init__()\n","        self.img_size = img_size\n","        self.disc = nn.Sequential(\n","            # input: N x img_channels x 64 x 64\n","            nn.Conv2d(img_channels+1, features_d, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            # _block(in_channels, out_channels, kernel_size, stride, padding)\n","            self._block(features_d, features_d * 2, 4, 2, 1),\n","            self._attention(features_d * 2),\n","            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n","            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n","            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n","            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n","        )\n","        self.embed = nn.Embedding(num_classes, img_size*img_size)\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False,)),\n","            nn.InstanceNorm2d(out_channels, affine=True),\n","            nn.LeakyReLU(0.2),\n","        )\n","    def _attention(self, in_channels):\n","        return Self_Attention_Layer(in_channels)\n","\n","    def forward(self, x, labels):\n","        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n","        x = torch.cat([x, embedding], dim=1) # N X C X img_size(H) X img_size(W) \n","        return self.disc(x)\n","\n","# generator network\n","class Generator(nn.Module):\n","    def __init__(self, channels_noise, img_channels, features_g, num_classes, img_size, embed_size,):\n","        super(Generator, self).__init__()\n","        self.img_size = img_size\n","        self.gen = nn.Sequential(\n","            # Input: N x channels_noise x 1 x 1\n","            self._block(channels_noise + embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n","            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n","            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n","            self._attention(features_g * 4),\n","            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n","            nn.ConvTranspose2d(\n","                features_g * 2, img_channels, kernel_size=4, stride=2, padding=1\n","            ),\n","            # Output: N x img_channels x 64 x 64\n","            nn.Tanh(),\n","        )\n","        self.embed = nn.Embedding(num_classes, embed_size)\n","\n","    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n","        return nn.Sequential(\n","            spectral_norm(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False,)),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),\n","        )\n","\n","    def _attention(self, in_channels):\n","        return Self_Attention_Layer(in_channels)\n","\n","    def forward(self, x, labels):\n","        # latent vector z: N X noise_dim X 1 X 1\n","        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n","        x = torch.cat([x, embedding], dim=1)\n","        return self.gen(x)\n","\n","# function for initializing weights of the neural network layers\n","def initialize_weights(model):\n","    for m in model.modules():\n","        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n","            nn.init.normal_(m.weight.data, 0.0, 0.02)\n","        if isinstance(m, (nn.Linear)):\n","            nn.init.xavier_uniform_(m.weight.data)"]},{"cell_type":"markdown","source":["## Training of the SAGAN\n","<img src='https://drive.google.com/uc?id=10cuYVwPZNHm2hQJMltlQqayRjjOrkTiQ'  height='250' width='600' ><br>\n","\n","<img src='https://drive.google.com/uc?id=1Lu4kCY3VITu7I9zFGj2jkGpWpjt1A3z8'  height='320' width='625' ><br>\n","**WGAN training**<br>[image source](https:///proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)"],"metadata":{"id":"YK-SM-cU--5w"}},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LPWBx6q9GvLj","outputId":"18cb1030-8a5d-4719-aac0-66acaf75405a","executionInfo":{"status":"error","timestamp":1651078301552,"user_tz":-330,"elapsed":1531339,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Generator(\n","  (gen): Sequential(\n","    (0): Sequential(\n","      (0): ConvTranspose2d(200, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (1): Sequential(\n","      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (2): Sequential(\n","      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (3): Self_Attention_Layer(\n","      (snconv1x1_theta): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_phi): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_g): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_attn): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (4): Sequential(\n","      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (5): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (6): Tanh()\n","  )\n","  (embed): Embedding(10, 100)\n",")\n","Discriminator(\n","  (disc): Sequential(\n","    (0): Conv2d(4, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    (1): LeakyReLU(negative_slope=0.2)\n","    (2): Sequential(\n","      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (3): Self_Attention_Layer(\n","      (snconv1x1_theta): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_phi): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_g): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n","      (snconv1x1_attn): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n","      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      (softmax): Softmax(dim=-1)\n","    )\n","    (4): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (5): Sequential(\n","      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n","      (2): LeakyReLU(negative_slope=0.2)\n","    )\n","    (6): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n","  )\n","  (embed): Embedding(10, 4096)\n",")\n","=> Loading checkpoint....\n","Starting Training Loop....\n","=> Saving checkpoint....\n","Epoch [0/100] Batch 100/782                   Disc Loss : -0.6669, Gen loss : -728.5729\n","Epoch [0/100] Batch 200/782                   Disc Loss : -1.3413, Gen loss : -1016.4845\n","Epoch [0/100] Batch 300/782                   Disc Loss : -4.2442, Gen loss : -1004.2518\n","Epoch [0/100] Batch 400/782                   Disc Loss : -1.8464, Gen loss : -1133.8834\n","Epoch [0/100] Batch 500/782                   Disc Loss : -0.8234, Gen loss : -912.9167\n","Epoch [0/100] Batch 600/782                   Disc Loss : -2.9409, Gen loss : -1446.4651\n","Epoch [0/100] Batch 700/782                   Disc Loss : -0.1416, Gen loss : -1528.5103\n","Epoch [1/100] Batch 100/782                   Disc Loss : -8.0565, Gen loss : -1003.1552\n","Epoch [1/100] Batch 200/782                   Disc Loss : -2.7879, Gen loss : -1214.4124\n","Epoch [1/100] Batch 300/782                   Disc Loss : -6.8462, Gen loss : -1499.8510\n","Epoch [1/100] Batch 400/782                   Disc Loss : -5.4725, Gen loss : -1292.7190\n","Epoch [1/100] Batch 500/782                   Disc Loss : -12.3392, Gen loss : -1080.1841\n","Epoch [1/100] Batch 600/782                   Disc Loss : -6.5772, Gen loss : -910.5816\n","Epoch [1/100] Batch 700/782                   Disc Loss : -1.5161, Gen loss : -1418.8372\n","Epoch [2/100] Batch 100/782                   Disc Loss : -2.3473, Gen loss : -1836.3141\n","Epoch [2/100] Batch 200/782                   Disc Loss : -2.0735, Gen loss : -1672.8047\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-262eba72202a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mcritic_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mcritic_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-18d6193fc9bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# N X C X img_size(H) X img_size(W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# generator network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-00efe12d757f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Phi path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnconv1x1_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/spectral_norm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, module, inputs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_power_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_solve_v_and_rescale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/spectral_norm.py\u001b[0m in \u001b[0;36mcompute_weight\u001b[0;34m(self, module, do_power_iteration)\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","channels_img = 3\n","batch_size = 64\n","img_size = 64\n","num_classes = 10\n","z_dim = 100\n","gen_embedding = 100\n","num_epochs = 100\n","critic_features = 16\n","gen_features = 16\n","disc_lr = 4e-4\n","gen_lr = 1e-4\n","critic_iterations = 5\n","lambda_gp = 10\n","\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]),\n","    ]\n",")\n","\n","dataset = datasets.CIFAR10(root=\"dataset/\", transform=transforms, download=True)\n","loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,)\n","\n","gen = Generator(z_dim, channels_img, gen_features, num_classes, img_size, gen_embedding).to(device)\n","print(gen)\n","critic = Discriminator(channels_img, critic_features, num_classes, img_size).to(device)\n","print(critic)\n","\n","initialize_weights(gen)\n","initialize_weights(critic)\n","\n","opt_gen = optim.Adam(gen.parameters(), lr=gen_lr, betas=(0.0, 0.9))\n","opt_critic = optim.Adam(critic.parameters(), lr=disc_lr, betas=(0.0, 0.9))\n","\n","if os.path.exists(os.path.join(\"./checkpoint/{}/\".format('sagan'), \"cifar10_sagan.pth.tar\")):\n","    checkpoint = torch.load(os.path.join(\"./checkpoint/{}/\".format('sagan'), \"cifar10_sagan.pth.tar\"))\n","    load_checkpoint(checkpoint, gen, critic)\n","\n","step = 0\n","\n","gen.train()\n","critic.train()\n","\n","Gen_losses = []\n","Disc_losses = []\n","fretchet_distances=[]\n","iter_list = []\n","\n","print(\"Starting Training Loop....\")\n","\n","for epoch in range(num_epochs):\n","    if epoch % 5 == 0:\n","        checkpoint = {'gen' : gen.state_dict(), 'critic' : critic.state_dict()}\n","        save_checkpoint(checkpoint)\n","    for batch_idx, (real, labels) in enumerate(loader):\n","        real = real.to(device)\n","        cur_batch_size = real.shape[0]\n","        labels=labels.to(device)\n","        # Train Critic: max E[critic(real)] - E[critic(fake)]\n","        # equivalent to minimizing the negative of that\n","        for _ in range(critic_iterations):\n","            noise = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n","            fake = gen(noise, labels)\n","            critic_real = critic(real, labels).reshape(-1)\n","            critic_fake = critic(fake, labels).reshape(-1)\n","            gp = gradient_penalty(critic, labels, real, fake, device=device)\n","            loss_critic = (-(torch.mean(critic_real) - torch.mean(critic_fake)) + lambda_gp * gp)\n","            critic.zero_grad()\n","            loss_critic.backward(retain_graph=True)\n","            opt_critic.step()\n","\n","        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n","        gen_fake = critic(fake, labels).reshape(-1)\n","        loss_gen = -torch.mean(gen_fake)\n","        gen.zero_grad()\n","        loss_gen.backward()\n","        opt_gen.step()\n","\n","        # Print losses \n","        if batch_idx % 100 == 0 and batch_idx > 0:\n","            print(\n","                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n","                  Disc Loss : {loss_critic:.4f}, Gen loss : {loss_gen:.4f}\"\n","            )\n","        # save images generated by the generator, generator and discriminator losses and fid score every 500 iterations\n","        if step % 500 == 0 and step > 0:\n","            with torch.no_grad():\n","                fake = gen(noise, labels)\n","                img_grid_fake = torchvision.utils.make_grid(fake[:64], normalize=True)\n","                if not os.path.exists(\"./outputs/{}/generated_images/\".format('sagan')):\n","                    os.makedirs(\"./outputs/{}/generated_images/\".format('sagan'))\n","                torchvision.utils.save_image(img_grid_fake, \"./outputs/{}/generated_images/img_{}.png\".format('sagan', str(step)),)\n","            iter_list.append(step)\n","            Gen_losses.append(loss_gen.item())\n","            Disc_losses.append(loss_critic.item())\n","            fretchet_distance=calculate_fretchet(real,fake,model)\n","            fretchet_distances.append(fretchet_distance.item())\n","\n","            losses = pd.DataFrame({\"Iteration No.\": iter_list, \"Generator loss\": Gen_losses, \"Discriminator loss\": Disc_losses, \"FID Score\": fretchet_distances})\n","\n","            if not os.path.exists(\"./outputs/{}/\".format('sagan')):\n","                os.makedirs(\"./outputs/{}/\".format('sagan'))\n","            losses.to_csv(\"./outputs/{}/output_data.csv\".format('sagan'))\n","\n","        step += 1"]},{"cell_type":"code","source":["# loading data from output_data.csv for plotting graphs \n","df = pd.read_csv(\"./outputs/{}/output_data.csv\".format('sagan'))\n","Iterations = df['Iteration No.']\n","Gen_Loss = df['Generator Loss']\n","Disc_Loss = df['Discriminator Loss']\n","FID = df['FID Score']"],"metadata":{"id":"QTB4H-hgDs6N","executionInfo":{"status":"aborted","timestamp":1651078301544,"user_tz":-330,"elapsed":522,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the generator losses\n","plt.xlabel('Iterations')\n","plt.ylabel('Generator Loss')\n","plt.plot(Iterations, Gen_Loss)"],"metadata":{"id":"uW-XSHvrDxE6","executionInfo":{"status":"aborted","timestamp":1651078301547,"user_tz":-330,"elapsed":524,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the discriminator losses\n","plt.xlabel('Iterations')\n","plt.ylabel('Discriminator Loss')\n","plt.plot(Iterations, Disc_Loss)"],"metadata":{"id":"YNc7DI54D2MR","executionInfo":{"status":"aborted","timestamp":1651078301549,"user_tz":-330,"elapsed":526,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotting the FID scores\n","plt.xlabel('Iterations')\n","plt.ylabel('FID Score')\n","plt.plot(Iterations, FID)"],"metadata":{"id":"MbjFMYU3D9Dq","executionInfo":{"status":"aborted","timestamp":1651078301550,"user_tz":-330,"elapsed":526,"user":{"displayName":"MOHIT KUMAR","userId":"13645489778962858445"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"SAGAN_MOHIT.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"83f5f80d96ae439ab892fbb70038dc23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed8c2b83d97e4779bb90e92b259e2158","IPY_MODEL_2a34e5fc98a948a1970fb3e21312ead6","IPY_MODEL_b393f1c2c7ff436cbe6ad16de656f6cd"],"layout":"IPY_MODEL_765ebc318005472fadee09bf13b98d4b"}},"ed8c2b83d97e4779bb90e92b259e2158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de641ad109914e16a62aea2fd9d30ec2","placeholder":"​","style":"IPY_MODEL_d09379a25ec84e758bc2582be69869e2","value":"100%"}},"2a34e5fc98a948a1970fb3e21312ead6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f4ae6c2c8aa48ab96476fc7b2e12d4d","max":108949747,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6120c46e4dae4dd5ab409e62768d02c5","value":108949747}},"b393f1c2c7ff436cbe6ad16de656f6cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2823982b920a4b35bd6b6b79ae02c6cc","placeholder":"​","style":"IPY_MODEL_a2321e34f376479ea4462896f7c5bfe0","value":" 104M/104M [00:00&lt;00:00, 144MB/s]"}},"765ebc318005472fadee09bf13b98d4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de641ad109914e16a62aea2fd9d30ec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d09379a25ec84e758bc2582be69869e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f4ae6c2c8aa48ab96476fc7b2e12d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6120c46e4dae4dd5ab409e62768d02c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2823982b920a4b35bd6b6b79ae02c6cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2321e34f376479ea4462896f7c5bfe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}